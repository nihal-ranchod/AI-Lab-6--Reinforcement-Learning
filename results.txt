The curves of the average rewards per episode for Q-Learning and SARSA show slightly different patterns.
The plot shows the SARSA algorithm to be slightly more stable, with less variance in the rewards across
episodes compared to Q-Learning. This difference could be attributed to the exploration-exploitation 
trade-off inherent in the two algorithms. Q-learning is an off-policy algorithm that always exploits the
current knowledge to make decisions, which can lead to higher variance in rewards. On the other hand, 
SARSA is an on-policy algorithm that takes into account the current policy (including exploration), 
which can lead to safer and more conservative decisions, especially in environments with high penalties.
Thus resulting in a more stable convergence. 